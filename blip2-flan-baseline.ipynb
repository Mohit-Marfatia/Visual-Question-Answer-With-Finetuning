{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-11T10:04:14.591552Z",
     "iopub.status.busy": "2025-05-11T10:04:14.590926Z",
     "iopub.status.idle": "2025-05-11T10:04:22.555127Z",
     "shell.execute_reply": "2025-05-11T10:04:22.553809Z",
     "shell.execute_reply.started": "2025-05-11T10:04:14.591523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading vqa dataset,inferencing on BLIP2-FLAN-T5-XL and calculating metrics: \n",
    "1. exact_match_accuracy\n",
    "2. relaxed_accuracy\n",
    "3. word_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-12T03:26:09.969Z",
     "iopub.execute_input": "2025-05-11T12:07:04.743087Z",
     "iopub.status.busy": "2025-05-11T12:07:04.742811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3136df1aff4a33b9e8396f2cb64fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Starting baseline evaluation...\n",
      "Loading dataset from /kaggle/input/vqa-dataset/vqa_dataset_gemini_final.csv\n",
      "Dataset loaded successfully with 19497 samples\n",
      "Columns: ['path', 'generated_question', 'generated_answer']\n",
      "\n",
      "==================================================\n",
      "Processing sample size: 3000 (3000 samples)\n",
      "==================================================\n",
      "Running inference on 3000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [1:25:00<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating predictions for sample size 3000...\n",
      "\n",
      "Results for sample size 3000:\n",
      "exact_match_accuracy: 0.0140\n",
      "relaxed_accuracy: 0.2465\n",
      "word_overlap: 0.2220\n",
      "Time taken: 5100.58 seconds\n",
      "Results for sample size 3000 saved to /kaggle/working/results/blip2_baseline_results_3000.csv\n",
      "\n",
      "==================================================\n",
      "Processing sample size: 7000 (7000 samples)\n",
      "==================================================\n",
      "Running inference on 7000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [3:18:01<00:00,  1.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating predictions for sample size 7000...\n",
      "\n",
      "Results for sample size 7000:\n",
      "exact_match_accuracy: 0.0150\n",
      "relaxed_accuracy: 0.2462\n",
      "word_overlap: 0.2237\n",
      "Time taken: 11881.64 seconds\n",
      "Results for sample size 7000 saved to /kaggle/working/results/blip2_baseline_results_7000.csv\n",
      "\n",
      "==================================================\n",
      "Processing sample size: 10000 (10000 samples)\n",
      "==================================================\n",
      "Running inference on 10000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8531/10000 [4:00:09<53:00,  2.17s/it]  "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "os.makedirs(\"/kaggle/working/results\", exist_ok=True)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model_name = \"Salesforce/blip2-flan-t5-xl\" \n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "def load_kaggle_vqa_dataset():\n",
    "    try:\n",
    "        csv_path = \"/kaggle/input/vqa-dataset/vqa_dataset_gemini_final.csv\"\n",
    "        print(f\"Loading dataset from {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Dataset loaded successfully with {len(df)} samples\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_inference(image_path, question):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                num_beams=5,\n",
    "                min_length=1,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for image {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def evaluate_predictions(predictions, ground_truths):\n",
    "    exact_matches = [pred.lower() == gt.lower() for pred, gt in zip(predictions, ground_truths)]\n",
    "    accuracy = sum(exact_matches) / len(exact_matches)\n",
    "\n",
    "    relaxed_matches = [gt.lower() in pred.lower() or pred.lower() in gt.lower() \n",
    "                       for pred, gt in zip(predictions, ground_truths)]\n",
    "    relaxed_accuracy = sum(relaxed_matches) / len(relaxed_matches)\n",
    "\n",
    "    def word_overlap_score(pred, gt):\n",
    "        if not pred or not gt:\n",
    "            return 0.0\n",
    "        pred_words = set(pred.lower().split())\n",
    "        gt_words = set(gt.lower().split())\n",
    "        return len(pred_words.intersection(gt_words)) / len(gt_words) if gt_words else 0.0\n",
    "\n",
    "    word_overlap_scores = [word_overlap_score(pred, gt) for pred, gt in zip(predictions, ground_truths)]\n",
    "    avg_word_overlap = sum(word_overlap_scores) / len(word_overlap_scores)\n",
    "\n",
    "    return {\n",
    "        \"exact_match_accuracy\": accuracy,\n",
    "        \"relaxed_accuracy\": relaxed_accuracy,\n",
    "        \"word_overlap\": avg_word_overlap\n",
    "    }\n",
    "\n",
    "def run_progressive_baseline_evaluation(sample_sizes=[3000, 7000, 'full']):\n",
    "    start_time_total = time.time()\n",
    "    full_df = load_kaggle_vqa_dataset()\n",
    "    if full_df is None:\n",
    "        print(\"Failed to load dataset. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_size = len(full_df)\n",
    "    actual_sample_sizes = [size if size != 'full' else total_size for size in sample_sizes]\n",
    "    actual_sample_sizes = sorted([min(size, total_size) for size in actual_sample_sizes if isinstance(size, (int, float)) or size == total_size])\n",
    "\n",
    "    all_metrics = {}\n",
    "    all_sample_times = {}\n",
    "    cumulative_results = pd.DataFrame(columns=['image_id', 'question', 'ground_truth', 'prediction', 'sample_size'])\n",
    "\n",
    "    for sample_size in actual_sample_sizes:\n",
    "        sample_name = 'full' if sample_size == total_size else str(sample_size)\n",
    "        print(f\"\\n{'='*50}\\nProcessing sample size: {sample_name} ({sample_size} samples)\\n{'='*50}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        current_df = full_df.sample(sample_size, random_state=42) if sample_size < total_size else full_df\n",
    "\n",
    "        predictions, ground_truths, image_ids, questions = [], [], [], []\n",
    "\n",
    "        print(f\"Running inference on {sample_size} samples...\")\n",
    "        for idx, row in tqdm(current_df.iterrows(), total=len(current_df)):\n",
    "            image_path = row['path']\n",
    "            question = row['generated_question']\n",
    "            ground_truth = row['generated_answer']\n",
    "\n",
    "            if not image_path.startswith('/kaggle'):\n",
    "                image_path = os.path.join('/kaggle/input/vqa-dataset/req-images/', image_path)\n",
    "\n",
    "            if not isinstance(question, str) or not isinstance(ground_truth, str):\n",
    "                continue\n",
    "\n",
    "            prediction = run_inference(image_path, question)\n",
    "            predictions.append(prediction)\n",
    "            ground_truths.append(ground_truth)\n",
    "            image_ids.append(os.path.basename(image_path))\n",
    "            questions.append(question)\n",
    "\n",
    "        print(f\"Evaluating predictions for sample size {sample_name}...\")\n",
    "        metrics = evaluate_predictions(predictions, ground_truths)\n",
    "        all_metrics[sample_size] = metrics\n",
    "        elapsed_time = time.time() - start_time\n",
    "        all_sample_times[sample_size] = elapsed_time\n",
    "\n",
    "        print(f\"\\nResults for sample size {sample_name}:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'image_id': image_ids,\n",
    "            'question': questions,\n",
    "            'ground_truth': ground_truths,\n",
    "            'prediction': predictions,\n",
    "            'sample_size': [sample_name] * len(predictions)\n",
    "        })\n",
    "\n",
    "        cumulative_results = pd.concat([cumulative_results, results_df], ignore_index=True)\n",
    "        sample_results_path = f\"/kaggle/working/results/blip2_baseline_results_{sample_name}.csv\"\n",
    "        results_df.to_csv(sample_results_path, index=False)\n",
    "        print(f\"Results for sample size {sample_name} saved to {sample_results_path}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame([all_metrics[size] for size in actual_sample_sizes], \n",
    "                              index=[str(size) if size != total_size else 'full' for size in actual_sample_sizes])\n",
    "    metrics_df['sample_size'] = [str(size) if size != total_size else 'full' for size in actual_sample_sizes]\n",
    "    metrics_df['time_seconds'] = [all_sample_times[size] for size in actual_sample_sizes]\n",
    "\n",
    "    metrics_path = \"/kaggle/working/results/blip2_baseline_all_metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_path)\n",
    "    print(f\"\\nAll metrics saved to {metrics_path}\")\n",
    "\n",
    "    all_results_path = \"/kaggle/working/results/blip2_baseline_all_results.csv\"\n",
    "    cumulative_results.to_csv(all_results_path, index=False)\n",
    "    print(f\"All results saved to {all_results_path}\")\n",
    "\n",
    "    plot_metrics(all_metrics, actual_sample_sizes, total_size)\n",
    "\n",
    "    total_time = time.time() - start_time_total\n",
    "    print(f\"\\nTotal evaluation time: {total_time:.2f} seconds\")\n",
    "    return all_metrics, cumulative_results\n",
    "\n",
    "def plot_metrics(all_metrics, sample_sizes, total_size):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metric_names = list(next(iter(all_metrics.values())).keys())\n",
    "    x_labels = [str(size) if size != total_size else 'full' for size in sample_sizes]\n",
    "\n",
    "    for metric in metric_names:\n",
    "        metric_values = [all_metrics[size][metric] for size in sample_sizes]\n",
    "        plt.plot(x_labels, metric_values, marker='o', linewidth=2, label=metric)\n",
    "\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('BLIP-2 Performance Metrics Across Sample Sizes')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = \"/kaggle/working/results/blip2_metrics_by_sample_size.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Metrics plot saved to {plot_path}\")\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting baseline evaluation...\")\n",
    "    sample_sizes = [3000, 7000, 10000, 'full']\n",
    "    all_metrics, all_results = run_progressive_baseline_evaluation(sample_sizes=sample_sizes)\n",
    "    print(\"Progressive baseline evaluation completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7387379,
     "sourceId": 11767209,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
